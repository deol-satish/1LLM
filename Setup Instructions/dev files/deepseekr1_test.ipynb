{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Local model path\n",
    "model_path = r'C:\\Users\\deols\\OneDrive\\Documents\\GitHub\\1LLM\\downloaded_plms\\deepseek\\base'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(device)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=100)\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, LlamaConfig\n",
    "\n",
    "\n",
    "# Path to the locally saved model\n",
    "model_path = r'C:\\Users\\deols\\OneDrive\\Documents\\GitHub\\1LLM\\downloaded_plms\\deepseek\\base'\n",
    "\n",
    "# Load the original config and adjust rope_scaling\n",
    "config = LlamaConfig.from_pretrained(model_path)\n",
    "# config.rope_scaling = {\"type\": \"linear\", \"factor\": 2.0}  # Modify this as needed\n",
    "\n",
    "# Load tokenizer (ensure the vocab and sp_model are in model_path)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Load model with modified config\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LlamaModel.from_pretrained(model_path, config=config, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(device)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llm_framework.plm_special.models.deepseek import DeepseekV3Model\n",
    "\n",
    "# Local model path\n",
    "model_path = r'C:\\Users\\deols\\OneDrive\\Documents\\GitHub\\1LLM\\downloaded_plms\\deepseek\\base'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = DeepseekV3Model.from_pretrained(model_path, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(device)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=100)\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizerFast'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Entered custom llama model\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6b04e611f648b893963a8b0c077b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from llm_framework.plm_special.models.llama3 import LlamaModel\n",
    "from transformers import LlamaConfig, LlamaTokenizerFast\n",
    "\n",
    "\n",
    "# Path to the locally saved model\n",
    "model_path = r'C:\\Users\\deols\\OneDrive\\Documents\\GitHub\\1LLM\\downloaded_plms\\deepseek\\base'\n",
    "\n",
    "# Load the original config and adjust rope_scaling\n",
    "config = LlamaConfig.from_pretrained(model_path)\n",
    "# config.rope_scaling = {\"type\": \"linear\", \"factor\": 2.0}  # Modify this as needed\n",
    "\n",
    "# Load tokenizer (ensure the vocab and sp_model are in model_path)\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Load model with modified config\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LlamaModel.from_pretrained(model_path, config=config, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(device)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ullmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
