{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deols\\anaconda3\\envs\\ullmenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers import LlamaConfig, LlamaTokenizer, LlamaModel, LlamaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.49s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain quantum computing in simple terms. What is the difference between classical and quantum computing?\n",
      "What is quantum computing? How is it different from classical computing?\n",
      "How to explain quantum computing in simple terms? What is the difference between classical and quantum computing?\n",
      "What is quantum computing? How is it different from classical computing?\n",
      "What is quantum computing? How is it different from classical computing?\n",
      "What is quantum computing? How is it different from classical computing?\n",
      "What is quantum computing? How is it different from\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaConfig\n",
    "\n",
    "# Path to the locally saved model\n",
    "model_path = r'C:\\Users\\deols\\OneDrive\\Documents\\GitHub\\1LLM\\downloaded_plms\\llama3\\base'\n",
    "\n",
    "# Load original config and adjust rope_scaling\n",
    "config = LlamaConfig.from_pretrained(model_path)\n",
    "config.rope_scaling = {\"type\": \"linear\", \"factor\": 2.0}  # Modify this as needed\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model with modified config\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(device)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=100)\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain quantum computing in simple terms. What is quantum computing?\n",
      "What is quantum computing? How does it work? What are its advantages and disadvantages?\n",
      "Quantum computing is a field of computer science that uses quantum mechanics to perform computations. Quantum computing is different from traditional computing in several ways. First, it uses quantum bits (qubits) instead of classical bits. A qubit can be in a superposition of states, meaning it can be in multiple states at once. This allows for more\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast, LlamaConfig\n",
    "\n",
    "# Path to the locally saved model\n",
    "model_path = r'C:\\Users\\deols\\OneDrive\\Documents\\GitHub\\1LLM\\downloaded_plms\\llama3\\base'\n",
    "\n",
    "# Load the original config and adjust rope_scaling\n",
    "config = LlamaConfig.from_pretrained(model_path)\n",
    "# config.rope_scaling = {\"type\": \"linear\", \"factor\": 2.0}  # Modify this as needed\n",
    "\n",
    "# Load tokenizer (ensure the vocab and sp_model are in model_path)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Load model with modified config\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(device)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=100)\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Decode output\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43moutput\u001b[49m[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaModel, PreTrainedTokenizerFast, LlamaConfig\n",
    "\n",
    "# Path to the locally saved model\n",
    "model_path = r'C:\\Users\\deols\\OneDrive\\Documents\\GitHub\\1LLM\\downloaded_plms\\llama3\\base'\n",
    "\n",
    "# Load the original config and adjust rope_scaling\n",
    "config = LlamaConfig.from_pretrained(model_path)\n",
    "# config.rope_scaling = {\"type\": \"linear\", \"factor\": 2.0}  # Modify this as needed\n",
    "\n",
    "# Load tokenizer (ensure the vocab and sp_model are in model_path)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Load model with modified config\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LlamaModel.from_pretrained(model_path, config=config, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(device)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "Entered custom llama model\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce9ec75ac7f4a7588bf192046c45137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from llm_framework.plm_special.models.llama3 import LlamaModel\n",
    "from transformers import PreTrainedTokenizerFast, LlamaConfig\n",
    "\n",
    "\n",
    "# Path to the locally saved model\n",
    "model_path = r'C:\\Users\\deols\\OneDrive\\Documents\\GitHub\\1LLM\\downloaded_plms\\llama3\\base'\n",
    "\n",
    "# Load the original config and adjust rope_scaling\n",
    "config = LlamaConfig.from_pretrained(model_path)\n",
    "# config.rope_scaling = {\"type\": \"linear\", \"factor\": 2.0}  # Modify this as needed\n",
    "\n",
    "# Load tokenizer (ensure the vocab and sp_model are in model_path)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Load model with modified config\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LlamaModel.from_pretrained(model_path, config=config, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "model.to(device)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ullmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
